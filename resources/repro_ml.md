# Reproducibility in Machine Learning - List of resources

## :page_facing_up: Journal Articles
Click the article you are interested in to read the abstract.
Articles are ordered by release date.

<details>
  <summary>
  <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2655335/IEEE_Systems_Supporting_Reproducibility-4.pdf?sequence=1">
  Out-of-the-box reproducibility: A survey of machine learning platforms</a>
  (2019) by Isdahl, R. and Gundersen, O.E.; 
  in 15th international conference on eScience.
  </summary>

> Even machine learning experiments that are fully
conducted on computers are not necessarily reproducible. An
increasing number of open source and commercial, closed source
machine learning platforms are being developed that help address
this problem. However, there is no standard for assessing and
comparing which features are required to fully support reproducibility. 
We propose a quantitative method that alleviates this
problem. Based on the proposed method we assess and compare
the current state of the art machine learning platforms for how
well they support making empirical results reproducible. Our
results show that BEAT and Floydhub have the best support for
reproducibility with Codalab and Kaggle as close contenders.
The most commonly used machine learning platforms provided
by the big tech companies have poor support for reproducibility.
</details>

<details>
  <summary>
  <a href="https://www.aaai.org/GuideBook2018/17248-73943-GB.pdf">
  State of the art: Reproducibility in artificial intelligence</a>
  (2018) by Gundersen, O.E. and Kjensmo, S.; 
  in Proceedings of the AAAI Conference on Artificial Intelligence.
  </summary>

> Background: Research results in artificial intelligence (AI)
are criticized for not being reproducible. 
Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. 
Hypotheses: 1) AI research is not
documented well enough to reproduce the reported results. 2)
Documentation practices have improved over time. Method:
The literature is reviewed and a set of variables that should be
documented to enable reproducibility are grouped into three
factors: Experiment, Data and Method. The metrics describe
how well the factors have been documented for a paper. 
A total of 400 research papers from the conference series IJCAI
and AAAI have been surveyed using the metrics. Findings:
None of the papers document all of the variables. The metrics
show that between 20% and 30% of the variables for each factor are documented. 
One of the metrics show statistically significant increase over time while the others show no change.
Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time
is found. Conclusion: Both hypotheses are supported.
</details>


## :white_check_mark: Checklists

## :checkered_flag: Challenges

## :card_file_box: Other tools