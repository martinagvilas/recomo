# Reproducibility in Machine Learning - List of resources

## :page_facing_up: Journal Articles
Click the article to read the abstract.
Articles are ordered by release date.

<details>
  <summary>
  <a href="https://arxiv.org/pdf/2003.12206.pdf">
  Improving reproducibility in machine learning research</a>
  (2020) by Pineau, J., Vincent-Lamarre, P., Sinha, K., Larivière, V.,
  Beygelzimer,  A., d'Alché-Buc, F., Fox, E. and Larochelle, H.;
  in arXiv preprint
  </summary>

> One of the challenges in machine learning research is to ensure that presented and published
results are sound and reliable. Reproducibility, that is obtaining similar results as presented
in a paper or talk, using the same code and data (when available), is a necessary step to
verify the reliability of research findings. Reproducibility is also an important step to
promote open and accessible research, thereby allowing the scientific community to quickly
integrate new findings and convert ideas to practice. Reproducibility also promotes the use
of robust experimental workflows, which potentially reduce unintentional errors. In 2019,
the Neural Information Processing Systems (NeurIPS) conference, the premier international
conference for research in machine learning, introduced a reproducibility program, designed
to improve the standards across the community for how we conduct, communicate, and
evaluate machine learning research. The program contained three components: a code
submission policy, a community-wide reproducibility challenge, and the inclusion of the
Machine Learning Reproducibility checklist as part of the paper submission process. In
this paper, we describe each of these components, how it was deployed, as well as what we
were able to learn from this initiative.
</details>

<details>
  <summary>
  <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2655335/IEEE_Systems_Supporting_Reproducibility-4.pdf?sequence=1">
  Out-of-the-box reproducibility: A survey of machine learning platforms</a>
  (2019) by Isdahl, R. and Gundersen, O.E.; 
  in 15th international conference on eScience.
  </summary>

> Even machine learning experiments that are fully
conducted on computers are not necessarily reproducible. An
increasing number of open source and commercial, closed source
machine learning platforms are being developed that help address
this problem. However, there is no standard for assessing and
comparing which features are required to fully support reproducibility. 
We propose a quantitative method that alleviates this
problem. Based on the proposed method we assess and compare
the current state of the art machine learning platforms for how
well they support making empirical results reproducible. Our
results show that BEAT and Floydhub have the best support for
reproducibility with Codalab and Kaggle as close contenders.
The most commonly used machine learning platforms provided
by the big tech companies have poor support for reproducibility.
</details>

<details>
  <summary>
  <a href="https://www.aaai.org/GuideBook2018/17248-73943-GB.pdf">
  State of the art: Reproducibility in artificial intelligence</a>
  (2018) by Gundersen, O.E. and Kjensmo, S.; 
  in Proceedings of the AAAI Conference on Artificial Intelligence.
  </summary>

> Background: Research results in artificial intelligence (AI)
are criticized for not being reproducible. 
Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. 
Hypotheses: 1) AI research is not
documented well enough to reproduce the reported results. 2)
Documentation practices have improved over time. Method:
The literature is reviewed and a set of variables that should be
documented to enable reproducibility are grouped into three
factors: Experiment, Data and Method. The metrics describe
how well the factors have been documented for a paper. 
A total of 400 research papers from the conference series IJCAI
and AAAI have been surveyed using the metrics. Findings:
None of the papers document all of the variables. The metrics
show that between 20% and 30% of the variables for each factor are documented. 
One of the metrics show statistically significant increase over time while the others show no change.
Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time
is found. Conclusion: Both hypotheses are supported.
</details>


## :white_check_mark: Checklists

## :checkered_flag: Challenges

## :card_file_box: Other tools